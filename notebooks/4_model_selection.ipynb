{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf77e97-6614-4a61-b250-58e33c59f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c15442d-78b3-4de3-b3d7-473c30a3fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffcd108-319f-4220-ab2e-eb0027358184",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d2633f-d0b5-4ce7-b0d7-742d94f4c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set baseline: 0.5\n",
      "Test set baseline: 0.5\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"../data/corpus.csv\").sample(frac=1).reset_index(drop=True) # shuffle data \n",
    "\n",
    "target = \"satire\"\n",
    "features = [\"title\", \"article_text\"]\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(\n",
    "    corpus, random_state=43, stratify=corpus[target]\n",
    ")\n",
    "\n",
    "train_dummy = max([train_corpus[target].sum(), 1.0 - train_corpus[target].sum()])\n",
    "test_dummy = max([test_corpus[target].sum(), 1.0 - test_corpus[target].sum()])\n",
    "\n",
    "print(f\"Train set baseline: {train_dummy / len(train_corpus)}\")\n",
    "print(f\"Test set baseline: {test_dummy / len(test_corpus)}\")\n",
    "\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e014f8-507b-4d85-9a77-e34d03ab0717",
   "metadata": {},
   "source": [
    "First things first, the data is loaded in, split into train and test sets, and the baseline performance is calculated. Both the training and testing sets are found to have an equal balance of the satire and non-satire categories, so baseline performance is 50.0% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16ea84-a14b-4708-8b8e-4b6ef885af32",
   "metadata": {},
   "source": [
    "The following cells declare a few functions to be used in preprocessing. The first will loop through each token in each article and convert it to a lemma, returning the lemmatized article with punctuation, whitespace, stop words, and out-of-value tokens removed. The next will calculate the \"imbalance\" scores defined in the previous notebook (specifically, their absolute values) and return a dictionary for threshold filtering. The third function accepts the corpus, the lemma imbalance scores, and the threshold to include and returns the corpus with all lemmas with too high of a score removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b029e0a1-7e66-44c7-93de-66257c107fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def lemmatize(corpus, nlp):\n",
    "    lemmatized_lists = []\n",
    "    for article in corpus:\n",
    "        doc = nlp(article)\n",
    "\n",
    "        lemmatized = [\n",
    "            str(token.lemma_).lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_oov\n",
    "            and not token.is_space\n",
    "        ]\n",
    "\n",
    "        lemmatized_lists.append(lemmatized)\n",
    "\n",
    "    return [\" \".join(lemma_list) for lemma_list in lemmatized_lists]\n",
    "\n",
    "\n",
    "def lemma_scores(lemmatized, y):\n",
    "    lemma_counts = {}\n",
    "    y = np.array(y)\n",
    "    for n, lemmas in enumerate(lemmatized):\n",
    "        for lemma in lemmas.split(\" \"):\n",
    "            if lemma not in lemma_counts:\n",
    "                lemma_counts[lemma] = [0, 0]\n",
    "            lemma_counts[lemma][y[n]] += 1\n",
    "\n",
    "    lemma_scores = {\n",
    "        lemma: np.log(value[0] + value[1])\n",
    "        * np.abs(value[0] - value[1])\n",
    "        / (value[0] + value[1])\n",
    "        for lemma, value in lemma_counts.items()\n",
    "    }\n",
    "\n",
    "    return lemma_scores\n",
    "\n",
    "\n",
    "def score_filtering(lemmatized_corpus, lemma_scores, threshold):\n",
    "    filtered_corpus = [\n",
    "        \" \".join(\n",
    "            [\n",
    "                lemma\n",
    "                for lemma in lemmas.split(\" \")\n",
    "                if lemma_scores.get(lemma, 0) <= threshold\n",
    "            ]\n",
    "        )\n",
    "        for lemmas in lemmatized_corpus\n",
    "    ]\n",
    "\n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439487e5-ae81-449c-91d0-a75777faa358",
   "metadata": {},
   "source": [
    "The data is lemmatized and \"scored\" exactly once before all the grid search fitting, as this is not a process that needs to be repeated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369f3a73-31c4-4695-a915-b25624f7cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[\"title\"] = lemmatize(train_corpus[\"title\"], nlp)\n",
    "train_corpus[\"article_text\"] = lemmatize(train_corpus[\"article_text\"], nlp)\n",
    "\n",
    "title_lemma_scores = lemma_scores(train_corpus[\"title\"], train_corpus[\"satire\"])\n",
    "article_lemma_scores = lemma_scores(\n",
    "    train_corpus[\"article_text\"], train_corpus[\"satire\"]\n",
    ")\n",
    "\n",
    "test_corpus[\"title\"] = lemmatize(test_corpus[\"title\"], nlp)\n",
    "test_corpus[\"article_text\"] = lemmatize(test_corpus[\"article_text\"], nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87bb09-11fc-4ecb-896d-5032e8f95c99",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Below, I will construct an instance of StackingClassifier, using two estimators on the base which feed into the output estimator. One estimator will receive the title, and the other will receive the article text. Designing the model in this way grants a few advantages:\n",
    "\n",
    "1. It will allow me to easily interpret how important the titles are compared to the article\n",
    "2. It will keep the titles and article separate, so that if one is considerably easier to identify than the other that information is presented in an isolated fashion\n",
    "3. When score thresholds are applied, words aren't removed from the title (what precious few there are) simply because it is common in the article body\n",
    "\n",
    "# Grid Search - Wide\n",
    "\n",
    "The following cell declares all sets of parameters I intend to search over to find the ideal model. In this section, few arguments are modified, and it is more about finding the best classifier and vectorizer than it is about finding the best *version* of any classifier or vectorizer. A few preliminary tests were run prior to this, and in all cases unigrams led to better fits than unigrams and bigrams, thus only unigrams will be used in the vectorizers. Similarly, ensemble models always outperformed logistic regression and naive Bayes, so the base models will only check try random forests and extra trees. Similarly, to avoid variance and to have strong interpretability, the model that the forest feeds into will only *not* use ensemble models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5c5817-0a53-49de-a321-4453e6ac8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizers\n",
    "cv = CountVectorizer()\n",
    "cv_params = {\n",
    "    \"<vectorizer>\": [cv],\n",
    "}\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_params = {\n",
    "    \"<vectorizer>\": [tfidf],\n",
    "}\n",
    "\n",
    "# classifier models\n",
    "logr = LogisticRegression()\n",
    "logr_params = {\n",
    "    \"<model>\": [logr],\n",
    "    \"<model>__C\": np.geomspace(1e-2, 1e2, 5),\n",
    "    \"<model>__max_iter\": [10000],\n",
    "}\n",
    "\n",
    "nb = BernoulliNB()\n",
    "nb_params = {\n",
    "    \"<model>\": [nb],\n",
    "    \"<model>__alpha\": [1e-5, 1e-1, 1.0],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "    \"<model>\": [rfc],\n",
    "    \"<model>__n_estimators\": [100],\n",
    "    \"<model>__max_depth\": [100],\n",
    "}\n",
    "\n",
    "etc = ExtraTreesClassifier()\n",
    "etc_params = {\n",
    "    \"<model>\": [etc],\n",
    "    \"<model>__n_estimators\": [100],\n",
    "    \"<model>__max_depth\": [100],\n",
    "}\n",
    "\n",
    "\n",
    "# the following loop creates a list of all parameter combinations to try\n",
    "# <model> and <vectorizer> will be replaced with the specific name for that part of the pipeline\n",
    "params = []\n",
    "vectorizers = [cv_params, tfidf_params]\n",
    "base_models = [rfc_params, etc_params]\n",
    "output_models = [logr_params, nb_params]\n",
    "for title_vectorizer in vectorizers:\n",
    "    title_vectorizer = {\n",
    "        k.replace(\"<vectorizer>\", \"title_pipe__vectorizer__vectorizer\"): v\n",
    "        for k, v in title_vectorizer.items()\n",
    "    }\n",
    "    for article_vectorizer in vectorizers:\n",
    "        article_vectorizer = {\n",
    "            k.replace(\"<vectorizer>\", \"article_pipe__vectorizer__vectorizer\"): v\n",
    "            for k, v in article_vectorizer.items()\n",
    "        }\n",
    "        for title_model in base_models:\n",
    "            title_model = {\n",
    "                k.replace(\"<model>\", \"title_pipe__model\"): v\n",
    "                for k, v in title_model.items()\n",
    "            }\n",
    "            for article_model in base_models:\n",
    "                article_model = {\n",
    "                    k.replace(\"<model>\", \"article_pipe__model\"): v\n",
    "                    for k, v in article_model.items()\n",
    "                }\n",
    "                for output_model in output_models:\n",
    "                    output_model = {\n",
    "                        k.replace(\"<model>\", \"final_estimator\"): v\n",
    "                        for k, v in output_model.items()\n",
    "                    }\n",
    "                    params.append(\n",
    "                        title_vectorizer\n",
    "                        | article_vectorizer\n",
    "                        | title_model\n",
    "                        | article_model\n",
    "                        | output_model\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db543a3a-f4d3-41e6-afd5-6082f8c5dacc",
   "metadata": {},
   "source": [
    "Below, the stacking classifier is declared. Two different pipelines are declared first, which will handle the vectorization and fitting of the titles and article bodies, respectively. These two estimators are then fed into the final estimator which makes the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c232eda4-153f-4b91-890f-3851b691ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            ColumnTransformer([(\"vectorizer\", cv, \"title\")]),\n",
    "        ),  # the estimators are declared with initial vectorizers and models\n",
    "        (\"model\", logr),  # but these will be replaced by the param dictionaries\n",
    "    ]\n",
    ")\n",
    "\n",
    "article_pipe = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", ColumnTransformer([(\"vectorizer\", cv, \"article_text\")])),\n",
    "        (\"model\", logr),\n",
    "    ]\n",
    ")\n",
    "\n",
    "estimators = [(\"title_pipe\", title_pipe), (\"article_pipe\", article_pipe)]\n",
    "final_estimator = logr\n",
    "\n",
    "model = StackingClassifier(estimators=estimators, final_estimator=final_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28382a3-4954-4b52-a0b1-2569d311eae4",
   "metadata": {},
   "source": [
    "Next, for each integer threshold from one to ten, the titles and articles are filtered separately to only allow balanced-enough lemmas as defined by the threshold. A grid search is run for all parameter combinations for all of these thresholds to get an idea of how bias and variance behave under different thresholds and what vectorizers and classifiers are most effective for each piece in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860edf46-d995-48db-a5a8-643961594404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0\n",
      "Cross-Validation Accuracy: 0.7263080526935324\n",
      "Test Set Accuracy: 0.7795809367296631\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=10.0, max_iter=10000), 'final_estimator__C': 10.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 2.0\n",
      "Cross-Validation Accuracy: 0.8410024245778093\n",
      "Test Set Accuracy: 0.8568200493015612\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=10.0, max_iter=10000), 'final_estimator__C': 10.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 3.0\n",
      "Cross-Validation Accuracy: 0.8931115493380479\n",
      "Test Set Accuracy: 0.9050944946589975\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(max_iter=10000), 'final_estimator__C': 1.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 4.0\n",
      "Cross-Validation Accuracy: 0.9264586625897492\n",
      "Test Set Accuracy: 0.928923582580115\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(max_iter=10000), 'final_estimator__C': 1.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 5.0\n",
      "Cross-Validation Accuracy: 0.9482332471990734\n",
      "Test Set Accuracy: 0.9496713229252259\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=0.1, max_iter=10000), 'final_estimator__C': 0.1, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 6.0\n",
      "Cross-Validation Accuracy: 0.9557651723701304\n",
      "Test Set Accuracy: 0.9570665571076418\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=10.0, max_iter=10000), 'final_estimator__C': 10.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 7.0\n",
      "Cross-Validation Accuracy: 0.9556283033114011\n",
      "Test Set Accuracy: 0.9574774034511093\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=0.1, max_iter=10000), 'final_estimator__C': 0.1, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 8.0\n",
      "Cross-Validation Accuracy: 0.9539851711508069\n",
      "Test Set Accuracy: 0.956861133935908\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(max_iter=10000), 'final_estimator__C': 1.0, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 9.0\n",
      "Cross-Validation Accuracy: 0.954258698231512\n",
      "Test Set Accuracy: 0.9599424815119145\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=0.1, max_iter=10000), 'final_estimator__C': 0.1, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n",
      "Threshold: 10.0\n",
      "Cross-Validation Accuracy: 0.9541221340036486\n",
      "Test Set Accuracy: 0.9599424815119145\n",
      "{'article_pipe__model': RandomForestClassifier(max_depth=100), 'article_pipe__model__max_depth': 100, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(), 'final_estimator': LogisticRegression(C=0.1, max_iter=10000), 'final_estimator__C': 0.1, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(max_depth=100), 'title_pipe__model__max_depth': 100, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer()}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "THRESHOLDS = np.linspace(1.0, 10.0, 10)\n",
    "\n",
    "X_test = test_corpus[[\"article_text\", \"title\"]].copy()\n",
    "y_test = test_corpus[\"satire\"].copy()\n",
    "\n",
    "for threshold in THRESHOLDS:\n",
    "    X = train_corpus[\n",
    "        [\"article_text\", \"title\"]\n",
    "    ].copy()  # X and y are declared directly from the full train data\n",
    "    y = train_corpus[\"satire\"].copy()  # so that previously removed lemmas are present\n",
    "\n",
    "    X[\"title\"] = score_filtering(\n",
    "        X[\"title\"], title_lemma_scores, threshold\n",
    "    )  # titles and articles are filtered\n",
    "    X[\"article_text\"] = score_filtering(\n",
    "        X[\"article_text\"], article_lemma_scores, threshold\n",
    "    )\n",
    "\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "\n",
    "    gs = GridSearchCV(model, params, n_jobs=-1)\n",
    "    gs.fit(X, y)\n",
    "    print(f\"Cross-Validation Accuracy: {gs.best_score_}\")\n",
    "    print(f\"Test Set Accuracy: {gs.score(X_test, y_test)}\")\n",
    "    print(gs.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfc7b2-17ee-4866-95eb-2a0f018a57e1",
   "metadata": {},
   "source": [
    "The above outputs make a few things clear:\n",
    "\n",
    "1. RandomForests are clearly superior to ExtraTrees on this data\n",
    "2. LogisticRegression is superior to NaiveBayes on this data\n",
    "3. Tfidf is superior when applied on the titles, and CountVectorizer performs better on the articles\n",
    "\n",
    "Other parameters are less clear, and need to be inspected more closely in a fine-tuned search in the next steps. As for the imbalance thresholds, higher thresholds resulted in a significant decrease in bias, but do not significantly increase variance as I expected it to. This is likely due to a combination of the fact that the corpus is quite large to begin with, and thus risk of overfitting was already quite small, and the fact that integer thresholds are quite large jumps. I suspect that there may be some interesting behavior that occurs between a threshold of 3 and 5, but this will not be explored in this analysis due to time constraints. Due to the fact that accuracy does not significantly increase above a threshold of 5, this is the value that will be used in the final model. \n",
    "\n",
    "The two cells below will implement these qualities, and explore more parameters that each piece of the model can accept. It is still not an especially exhaustive search, but model performance is already quite accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83394ed2-2127-4175-93f2-ab069ff55770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizers\n",
    "cv = CountVectorizer()\n",
    "cv_params = {\"<vectorizer>\": [cv], \"<vectorizer>__max_features\": [None, 10000]}\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_params = {\"<vectorizer>\": [tfidf], \"<vectorizer>__max_features\": [None, 1000]}\n",
    "\n",
    "# classifier models\n",
    "logr = LogisticRegression()\n",
    "logr_params = {\n",
    "    \"<model>\": [logr],\n",
    "    \"<model>__C\": np.geomspace(1e-2, 1, 9),\n",
    "    \"<model>__max_iter\": [10000],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "    \"<model>\": [rfc],\n",
    "    \"<model>__n_estimators\": [100],\n",
    "    \"<model>__max_depth\": [None, 100],\n",
    "}\n",
    "\n",
    "params = []\n",
    "vectorizers = [cv_params, tfidf_params]\n",
    "base_models = [rfc_params]\n",
    "output_models = [logr_params]\n",
    "for title_vectorizer in [tfidf_params]:\n",
    "    title_vectorizer = {\n",
    "        k.replace(\"<vectorizer>\", \"title_pipe__vectorizer__vectorizer\"): v\n",
    "        for k, v in title_vectorizer.items()\n",
    "    }\n",
    "    for article_vectorizer in [cv_params]:\n",
    "        article_vectorizer = {\n",
    "            k.replace(\"<vectorizer>\", \"article_pipe__vectorizer__vectorizer\"): v\n",
    "            for k, v in article_vectorizer.items()\n",
    "        }\n",
    "        for title_model in base_models:\n",
    "            title_model = {\n",
    "                k.replace(\"<model>\", \"title_pipe__model\"): v\n",
    "                for k, v in title_model.items()\n",
    "            }\n",
    "            for article_model in base_models:\n",
    "                article_model = {\n",
    "                    k.replace(\"<model>\", \"article_pipe__model\"): v\n",
    "                    for k, v in article_model.items()\n",
    "                }\n",
    "                for output_model in output_models:\n",
    "                    output_model = {\n",
    "                        k.replace(\"<model>\", \"final_estimator\"): v\n",
    "                        for k, v in output_model.items()\n",
    "                    }\n",
    "                    params.append(\n",
    "                        title_vectorizer\n",
    "                        | article_vectorizer\n",
    "                        | title_model\n",
    "                        | article_model\n",
    "                        | output_model\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20381496-81d6-455e-ae3e-2753b5feb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Cross-Validation Accuracy: 0.9528893745339605\n",
      "Test Set Accuracy: 0.952341824157765\n",
      "{'article_pipe__model': RandomForestClassifier(), 'article_pipe__model__max_depth': None, 'article_pipe__model__n_estimators': 100, 'article_pipe__vectorizer__vectorizer': CountVectorizer(max_features=10000), 'article_pipe__vectorizer__vectorizer__max_features': 10000, 'final_estimator': LogisticRegression(C=0.31622776601683794, max_iter=10000), 'final_estimator__C': 0.31622776601683794, 'final_estimator__max_iter': 10000, 'title_pipe__model': RandomForestClassifier(), 'title_pipe__model__max_depth': None, 'title_pipe__model__n_estimators': 100, 'title_pipe__vectorizer__vectorizer': TfidfVectorizer(), 'title_pipe__vectorizer__vectorizer__max_features': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = test_corpus[[\"article_text\", \"title\"]].copy()\n",
    "y_test = test_corpus[\"satire\"].copy()\n",
    "\n",
    "threshold = 5.0\n",
    "\n",
    "X = train_corpus[[\"article_text\", \"title\"]].copy()\n",
    "y = train_corpus[\"satire\"].copy()\n",
    "\n",
    "X[\"title\"] = score_filtering(X[\"title\"], title_lemma_scores, threshold)\n",
    "X[\"article_text\"] = score_filtering(X[\"article_text\"], article_lemma_scores, threshold)\n",
    "\n",
    "gs = GridSearchCV(model, params, n_jobs=-1, verbose=1)\n",
    "gs.fit(X, y)\n",
    "print(f\"Cross-Validation Accuracy: {gs.best_score_}\")\n",
    "print(f\"Test Set Accuracy: {gs.score(X_test, y_test)}\")\n",
    "print(gs.best_params_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3f963-d6ab-4813-a7aa-3d5438d9a664",
   "metadata": {},
   "source": [
    "The results are in, and the final model will implement a max_features count of 10,000 on the article vectorizer, and a regularization parameter of C = 0.316 for the logistic regression.\n",
    "\n",
    "(NOTE: This notebook was rerun, and do to me forgetting to define the random state in pandas .sample() method, these results changed slightly. Initial runs gave C = 1.0, and that is what is used in the conclusions notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
