{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfd39b6-f58c-4896-bb6c-305aa99916d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8022705e-f3f3-4658-8a9d-8120d0a2720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import bs4\n",
    "import sqlite3\n",
    "import newspaper\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from urllib3.exceptions import LocationParseError\n",
    "from multiprocessing import Pool\n",
    "from math import floor, ceil\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from subreddit_sql import create_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a575e-2d11-4502-a919-559339f43048",
   "metadata": {},
   "source": [
    "The first two steps in data cleaning will be determining if the content was submitted to the correct subreddit, and if that content has been submitted before. To handle the first problem, we'll determine if the domain of the article is a known satire website or not. The website [realorsatire.com](https://realorsatire.com/) maintains a curated list of websites which publish satirical articles. This website will be scraped and a list of domains will be formed. Anything in this list will be allowed in the r/TheOnion table, and anything not present will be considered appropriate for r/nottheonion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54568d8-4c85-42de-b699-0e08c3a4b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertable_to_int(string):\n",
    "    try:\n",
    "        x = int(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_satire_list():\n",
    "    satire_domains = []\n",
    "\n",
    "    # total number of pages available starting from page 2\n",
    "    source = \"https://realorsatire.com/websites-that-are/satire/page/\"\n",
    "    r = requests.get(source + \"2/\")\n",
    "\n",
    "    html = r.content\n",
    "    soup = bs4.BeautifulSoup(html)\n",
    "\n",
    "    words = soup.find(\"title\").get_text().split(\" \")\n",
    "    page_numbers = [int(word) for word in words if convertable_to_int(word)]\n",
    "    num_pages = max(page_numbers)\n",
    "\n",
    "    for n in range(1, num_pages + 1):\n",
    "        r = requests.get(source + f\"{n}/\")\n",
    "        html = r.content\n",
    "        soup = bs4.BeautifulSoup(html)\n",
    "\n",
    "        title_blocks = soup.find_all(\"a\", rel=\"bookmark\")\n",
    "        for tb in title_blocks:\n",
    "\n",
    "            satire_domains.append(tb.get_text().lower())\n",
    "\n",
    "    return satire_domains\n",
    "\n",
    "\n",
    "satire_list = create_satire_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37f8dd-dfdd-43af-8cf7-cc4625d2d7b6",
   "metadata": {},
   "source": [
    "From the r/TheOnion table, all posts with article content and appropriate domains will be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00049c13-eed8-46b0-bf97-a921b3252d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12946 entries, 0 to 12945\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         12946 non-null  object \n",
      " 1   article_text  12946 non-null  object \n",
      " 2   score         12824 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 303.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# from sql, return rows with article content from acceptable domains sorted by score\n",
    "def pull_onion(conn, domains):\n",
    "    sql_cmd = f\"\"\"SELECT\n",
    "                      title,\n",
    "                      article_text,\n",
    "                      score\n",
    "                  FROM theonion\n",
    "                  WHERE article_text IS NOT NULL AND\n",
    "                  ({\" OR \".join([f\"domain LIKE '%{d}%'\" for d in domains])})\n",
    "                  ORDER BY score DESC;\"\"\"\n",
    "\n",
    "    c = conn.cursor()\n",
    "    c.execute(sql_cmd)\n",
    "    rows = c.fetchall()\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "conn = create_connection(\"../data/reddit.db\")\n",
    "onion_df = pd.DataFrame(\n",
    "    pull_onion(conn, satire_list), columns=[\"title\", \"article_text\", \"score\"]\n",
    ")\n",
    "\n",
    "onion_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36330d17-1f68-4f10-bcf5-25b2c8fffb00",
   "metadata": {},
   "source": [
    "From r/nottheonion, the same criteria need to be met, but due to the sheer number of posts we will limit it to only fetching twice the number of posts that are retrieved from r/TheOnion. This will ensure that after data cleaning a sufficient number of posts are present in both. Furthermore, by sorting the posts by their score, we help ensure that the subset of posts we do fetch are the \"best\" representation of r/nottheonion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1339798b-e41c-4baa-996e-275ee4e1201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25892 entries, 0 to 25891\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         25892 non-null  object\n",
      " 1   article_text  25892 non-null  object\n",
      " 2   score         25892 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 607.0+ KB\n"
     ]
    }
   ],
   "source": [
    "def pull_not_onion(conn, domains, limit):\n",
    "    sql_cmd = f\"\"\"SELECT\n",
    "                      title,\n",
    "                      article_text, \n",
    "                      score\n",
    "                  FROM nottheonion\n",
    "                  WHERE article_text IS NOT NULL AND\n",
    "                  ({\" AND \".join([f\"domain NOT LIKE '%{d}%'\" for d in domains])})\n",
    "                  ORDER BY score DESC\n",
    "                  LIMIT {limit};\"\"\"\n",
    "\n",
    "    c = conn.cursor()\n",
    "    c.execute(sql_cmd)\n",
    "    rows = c.fetchall()\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "not_onion_df = pd.DataFrame(\n",
    "    pull_not_onion(conn, satire_list, 2 * len(onion_df)),\n",
    "    columns=[\"title\", \"article_text\", \"score\"],\n",
    ")\n",
    "\n",
    "not_onion_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d65e99-afc4-4ee3-b8e6-b100d8f13ae4",
   "metadata": {},
   "source": [
    "To actually remove duplicate posts is a tricky process. pandas provides a drop_duplicates() method, however this does not take into account typos titles with slightly changed wording. Therefore, I chose to use fuzzy matching to cross-check every title against every other title. This process is slow, but thorough. Using the library fuzzywuzzy, each title pair is given a score from 0 to 100, with 100 indicating identical titles. By setting a threshold of 98, posts that are very similar will be removed, but this will still leave us with sufficient data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2473469-3990-4817-8e25-42ef4eb192c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates within the onion titles\n",
    "# to help ensure only true duplicates are removed, we will use a very high threshold for similarity\n",
    "FUZZY_THRESHOLD = 98\n",
    "\n",
    "\n",
    "def remove_duplicate(df, column, threshold):\n",
    "    values = df[column]\n",
    "    N = len(values)\n",
    "    duplicates = {n: [] for n in range(len(values))}\n",
    "\n",
    "    for n, value in enumerate(values):\n",
    "        clear_output(wait=True)\n",
    "        possible_matches = process.extract(value, values)\n",
    "        for possible_match in possible_matches:\n",
    "            # value, score, index unpacking\n",
    "            v, s, i = possible_match\n",
    "            if i != n and s >= threshold:\n",
    "                duplicates[n].append(i)\n",
    "\n",
    "        print(f\"Values processed: {n + 1}/{N}\")\n",
    "\n",
    "    dupe_keys = []\n",
    "    for key in duplicates.keys():\n",
    "        if key not in dupe_keys:\n",
    "            for dupe in duplicates[key]:\n",
    "                dupe_keys.append(dupe)\n",
    "\n",
    "    df = df.drop(index=dupe_keys).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf8182-b1f8-48e9-a039-74124070fee9",
   "metadata": {},
   "source": [
    "The steps taken in this notebook will help ensure a few things. Firstly, there is no null data, and thus nothing to impute. The corpus is sufficiently large that this is an appropriate measure. Second, there is a high degree of confidence that all articles are unique, and therefore the data will have good variety for the model to learn. Lastly, by selecting a roughly equal number of posts from each (and further subsetting the data in the next notebook), a balanced corpus will be created.\n",
    "\n",
    "Finally, the data with duplicates removed will be saved to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f4683a-d592-4c64-a734-82deb55fad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values processed: 12946/12946\n"
     ]
    }
   ],
   "source": [
    "no_dupes_path = pathlib.Path(\"../data/duplicates_removed\")\n",
    "no_dupes_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "onion_df = remove_duplicate(onion_df, \"title\", FUZZY_THRESHOLD)\n",
    "onion_df.to_csv(\"../data/duplicates_removed/theonion.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb0a3c2-9e0a-4b94-8c00-f2773ad6c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values processed: 25892/25892\n"
     ]
    }
   ],
   "source": [
    "not_onion_df = remove_duplicate(not_onion_df, \"title\", FUZZY_THRESHOLD)\n",
    "not_onion_df.to_csv(\"../data/duplicates_removed/nottheonion.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
