{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a046df-fe66-4ec6-8376-4828f0d70dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383cb137-1b40-4ff6-9510-e6d5df153054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import bs4\n",
    "import sqlite3\n",
    "import newspaper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from urllib3.exceptions import LocationParseError\n",
    "from multiprocessing import Pool\n",
    "from math import floor, ceil\n",
    "from subreddit_sql import sqlize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324817fa-65bb-45cc-bf01-8119932be494",
   "metadata": {},
   "source": [
    "The following cell defines three functions which will act as a handler for the Pushshift API for data aggregation. As we want as many posts as possible, the data_aggregator function will start at the beginning of time (also known as 00:00 January 1st, 1970), fetches the first 500 posts, then repeats this process using the timestamp of the last post it fetched as the first time to check. This cycle continues while being rate-limited by submission_api_handler until there are no new posts received, in which case the loop automatically terminates. In the case that pushshift_fetcher receives an invalid status code, it will attempt to repeat the query. After ten consecutive failed attempts, the function will raise an error and terminate.\n",
    "\n",
    "Upon a successful download, the posts are stored loosely as individual json files with a name corresponding to their integer id. I made this choice because not every post will contain the same keys, and this method is robust against crashes as it references files in storage for looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390dbd46-9b97-4e36-96f5-ea9657e89620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushshift_fetcher(endpoint, params):\n",
    "    # send formatted request to pushshift api\n",
    "    base_url = \"https://api.pushshift.io/reddit/\"\n",
    "    api_url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def data_aggregator(subreddit):\n",
    "    # create directory to store json files\n",
    "    data_directory = \"../data/raw\"\n",
    "    p = pathlib.Path(f\"{data_directory}/{subreddit}\")\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # get list of currently existing files\n",
    "    # and get most recent post\n",
    "    all_files = list(p.glob(\"*.json\"))\n",
    "    if all_files:\n",
    "        last_fetched_file = all_files[np.argmax([int(f.stem) for f in all_files])]\n",
    "        with open(last_fetched_file, \"r\") as last:\n",
    "            content = json.load(last)\n",
    "            # get timestamp and id of most recent post\n",
    "            last_utc = content[\"created_utc\"] + 1\n",
    "            last_id = content[\"id\"]\n",
    "\n",
    "    else:\n",
    "        # if no posts are saved\n",
    "        # set timestamp and id as zero\n",
    "        last_utc = 0\n",
    "        last_id = 0\n",
    "\n",
    "    # specify params for requests.get\n",
    "    params = {\n",
    "        \"subreddit\": subreddit,\n",
    "        \"after\": last_utc,\n",
    "        \"size\": 500,\n",
    "        \"sort_type\": \"id\",\n",
    "    }\n",
    "    # get content\n",
    "    r = pushshift_fetcher(\"search/submission/\", params)\n",
    "\n",
    "    # if response code is not 200\n",
    "    # retry until it is or ten consecutive fails\n",
    "    fails = 0\n",
    "    while r.status_code != 200:\n",
    "        fails += 1\n",
    "        time.sleep(10)\n",
    "        r = pushshift_fetcher(\"search/submission/\", params)\n",
    "        if fails >= 10:\n",
    "            # not sure which error to use here\n",
    "            raise BaseException(\"Request failed ten consecutive times. Exiting.\")\n",
    "\n",
    "    # write each post returned to its own json file\n",
    "    data = r.json()[\"data\"]\n",
    "    if data:\n",
    "        for d in data:\n",
    "            # convert base 36 id to integer id\n",
    "            base36_id = d[\"id\"]\n",
    "            integer_id = int(base36_id, 36)\n",
    "            dump_path = p / f\"{integer_id}.json\"\n",
    "            with dump_path.open(mode=\"w+\") as new_file:\n",
    "                json.dump(d, new_file)\n",
    "\n",
    "        # if data has entries, return true\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        # if data is empty, return false\n",
    "        return False\n",
    "\n",
    "\n",
    "def submission_api_handler(subreddit):\n",
    "    rate_limiter = 1\n",
    "    is_new_content = True\n",
    "    while is_new_content:\n",
    "        is_new_content = data_aggregator(subreddit)\n",
    "        time.sleep(rate_limiter)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "submission_api_handler(\"theonion\")\n",
    "submission_api_handler(\"nottheonion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449a034-4ca0-4eb5-8b60-aaab955bd296",
   "metadata": {},
   "source": [
    "r/TheOnion and r/nottheonion are \"news\" aggregate subreddits. Users will share links to articles they find online, and occasionally videos. As such, there are very few text posts from users. This left me with three options: use only the titles of posts, incorporate the comments, or follow the links to get the text from the actual articles. I chose the latter. The function below iterates through all of the json files, gets the url to the article if it's available, and uses automatic webscraping with Newspaper3k to extract the article content. The article text is then saved as a txt file with the same name as its corresponding post json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde0bbf0-a801-44e8-96cd-cd1f51790f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_content_fetcher(path):\n",
    "    integer_id = path.stem\n",
    "    text_dump = pathlib.Path(path.parent / f\"{integer_id}.txt\")\n",
    "\n",
    "    if not text_dump.is_file():\n",
    "        js = path.open(mode=\"r\")\n",
    "        json_content = json.load(js)\n",
    "        url = json_content.get(\"url\")\n",
    "        try:\n",
    "            article = newspaper.Article(url=url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            content = article.text\n",
    "\n",
    "            if not content:\n",
    "                content = \"\"\n",
    "\n",
    "        except (TypeError, newspaper.ArticleException, LocationParseError):\n",
    "            # TypeError will occur when there is no url\n",
    "            # ArticleException will occur when the response forbids webscraping\n",
    "            # not sure what causes LocationParseError, but it's rare\n",
    "            content = \"\"\n",
    "\n",
    "        with text_dump.open(mode=\"w+\") as txt:\n",
    "            txt.write(content)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b66ec1-03e4-4345-8835-57529a23249f",
   "metadata": {},
   "source": [
    "Webscraping like this is an incredibly long process, and would taken well over a week on its own. Therefore, multiprocessing is used to reduce this time to under 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84962ec6-24a0-4e10-aad0-3805bbc1c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_directory = \"../data/raw/theonion\"\n",
    "onion_path = pathlib.Path(onion_directory)\n",
    "onion_glob = list(onion_path.glob(\"*.json\"))\n",
    "\n",
    "not_onion_directory = \"../data/raw/nottheonion\"\n",
    "not_onion_path = pathlib.Path(not_onion_directory)\n",
    "not_onion_glob = list(not_onion_path.glob(\"*.json\"))\n",
    "\n",
    "pool = Pool()\n",
    "not_onion_percent = 0\n",
    "for onion_percent in range(100):\n",
    "    # prints progress as percents\n",
    "    bot = floor(onion_percent * len(onion_glob) / 100.0)\n",
    "    top = ceil((onion_percent + 1) * len(onion_glob) / 100.0)\n",
    "\n",
    "    pool.map(article_content_fetcher, onion_glob[bot:top])\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Onion harvesting {onion_percent + 1}% complete\")\n",
    "    print(f\"Not Onion harvesting {not_onion_percent}% complete\")\n",
    "\n",
    "for not_onion_percent in range(100):\n",
    "    bot = floor(not_onion_percent * len(not_onion_glob) / 100.0)\n",
    "    top = ceil((not_onion_percent + 1) * len(not_onion_glob) / 100.0)\n",
    "\n",
    "    pool.map(article_content_fetcher, not_onion_glob[bot:top])\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Onion harvesting {onion_percent + 1}% complete\")\n",
    "    print(f\"Not Onion harvesting {not_onion_percent + 1}% complete\")\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59c338-8115-42b6-95b3-f0408d2ce5c4",
   "metadata": {},
   "source": [
    "Finally, up until now, all files have been stored loosely, with one example per file. The sheer number of posts (over 500,000) makes estimating total data volume tough. During data aggregation, I was worried that the magnitude of data could exceed what pandas could practically handle. Therefore, I created a SQL database in storage that I could write formatted data to. This would allow me to do the first round of data cleaning without having to load all of the data into memory (this fear turned out to be mostly unfounded, with only about 1.1 GB of data being stored total).\n",
    "\n",
    "The following function gets all possible keys found in any json file, creates a table with the same keys, and formats each file as a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc286c9-ed22-423d-bdd4-35cd4b42d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts SQLized: 17644/17644\r"
     ]
    }
   ],
   "source": [
    "sqlize(\"../data/reddit.db\", \"theonion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7a777e-1983-4e7d-859e-b911e9a4d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts SQLized: 487364/487364\r"
     ]
    }
   ],
   "source": [
    "sqlize(\"../data/reddit.db\", \"nottheonion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725bafd-9216-4760-bc94-66e6fd97288d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
