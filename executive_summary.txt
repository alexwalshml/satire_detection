Often times it seems reality is stranger than fiction, especially as of late. Headlines are getting more and more bizarre by the day. Things we read online can leave us scratching our heads about how such a thing ever came to be. Sometimes, it turns out, the things we read never even happened in the first place. Not because it's fake news, bad reporting, or propaganda, but because it was satire meant to lift your spirits (or, perhaps, to crush them). You've been pranked, duped, bamboozled, and you're left feeling like a fool. You've just eaten The Onion.

If only there was a way that you could have been spared of this frustration and embarassment, and enjoyed the jokes as they were intended. Or maybe you aren't the victim, but you know one. Somebody who just can't seem to separate fantasy from reality. Somebody who believes everything they read online. Somebody who just can't help it. But I, with the power of natural language processing, can help you, them, and anyone else who needs a bit of help sparing their sanity from the increasingly difficult task of determinig whether or not that article they're about to read is a cleverly constructed bit, or just plain old societal weirdness.

Using NLP techniques, it is possible to classify "news" articles as being real or satirical. The Reddit community has created two groups, r/TheOnion, for those who love a good joke, and r/nottheonion, for those news stories that just don't seem quite real, but unfortunately are. By using articles gathered from these two subreddits, the line between satire and reality can be drawn. Machine learning techniques can learn what to look for, and tell the user the true nature of what they're reading. Such a model could be implemented as a phone app that accepts a URL, a social media bot that checks your feed, or a browser extension to label your search results. This model is craftable and implementable, can identify satire to a high degree of accuracy, and will only get better over time. 

As a data scientist, I am well versed in the myriad of techniques that can be used in such a task. I have experience with effective webscraping, and will gather enough data to make an accurate model. Furthermore, I will implement state-of-the-art technologies to find the most effecting classification method, while still being fast and easy for a machine to compute. 

This model could be implemented in a number of ways, and would improve with each user. This model could even serve as a basis for performing something like fake news detection if it becomes developed enough, or identifying original sources for plagiarism. This model is achievable, applicable, and marketable.
